{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La feature selection, c'est quoi ? \n",
    "\n",
    "La feature selection, c'est le fait de choisir quelles variables explicatives on souhaite garder pour mettre en place notre modèle. On va chercher à limiter le nombre de colonnes en minimisant la perte d'informations provenant de la suppression des autres colonnes en éliminant les données redondantes ou inutiles pour notre étude. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pourquoi faire cette selection ? \n",
    "\n",
    "La feature selection a plusieurs intérêts :\n",
    "- simplifier le modèle pour le rendre plus facile à interpréter (lors de la mise en place du modèle et pour l'utilisateur)\n",
    "- réduire le temps d'entrainement et le temps d'exécution du programme\n",
    "- éviter la \"curse of dimensionality\" (fléau de la dimension):\n",
    "    Quand le nombre de features augmente, le nombre de données nécessaires à la généralisation augmente exponentiellement.  \n",
    "    Illustration de la curse of dimensionality : \n",
    "![Illustration de la curse of dimensionality](https://www.researchgate.net/profile/Xavier-Hadoux/publication/278786166/figure/fig9/AS:613951482101777@1523388848382/Illustration-of-the-Curse-of-dimensionality-showing-how-the-number-of-regions-of-a.png)\n",
    "- améliorer la généralisation en réduisant l'overfitting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quels sont les différents moyens de réaliser de la feature selection avec scikit ? \n",
    "\n",
    "Scikit met à disposition un module permettant de faire de la feature selection : ```sklearn.feature_selection```.\n",
    "On va parcourir l'ensemble des outils mis à disposition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sélection des features ayant une variance suffisante\n",
    "#### [VarianceThreshhold](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html#sklearn.feature_selection.VarianceThreshold)\n",
    "\n",
    "Cet algorithme permet de retirer toutes les colonnes ayant une variance faible, inférieure à un seuil  ```threshhold``` qu'on peut fixer. Il ne s'applique que aux variables explicatives et peut donc être également utilisé en apprentissage non supervisé.  \n",
    "L'intérêt est de se débarasser de colonnes ayant (quasiment) tout le temps la même valeur pour toutes les lignes et qui n'apportent donc pas d'indication suffisamment pertinente pour obtenir la valeur cible.  \n",
    "Exemple d'un selecteur permettant de retirer les colonnes qui contiennent plus de 80% de la même valeur: \n",
    "```python\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sélection univariée des features\n",
    "\n",
    "On peut sélectionner les colonnes suivant des tests statistiques univariés qui utiliseront la dépendance entre nos colonnes explicatives et la variable cible. Celà permet de garder les colonnes les plus proches et liés à la variable cible. \n",
    "\n",
    "#### [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest)\n",
    "\n",
    "Garde uniquement les ```K``` features ayant le plus haut score. \n",
    "\n",
    "#### [SelectPercentile](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html#sklearn.feature_selection.SelectPercentile)\n",
    "\n",
    "Garde uniquement un pourcentage donné des features ayant le plus haut score.\n",
    "\n",
    "#### [SelectFpr](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFpr.html#sklearn.feature_selection.SelectFpr)\n",
    "\n",
    "Garde les colonnes ayant une pvalue sous un seuil donné basé sur un test de taux de faux positifs (FPR).\n",
    "\n",
    "#### [SelectFdr](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFdr.html#sklearn.feature_selection.SelectFdr)\n",
    "\n",
    "Garde les colonnes ayant une pvalue sous un seuil donné basé sur le False Discovery Rate (FDR). Celà permet d'éliminer les colonnes ne respectant pas l'hypothèse nulle.\n",
    "\n",
    "#### [SelectFwe](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFwe.html#sklearn.feature_selection.SelectFwe)\n",
    "\n",
    "Garde les colonnes ayant une pvalue sous un seuil donné basé sur le Family-wise Error Rate (Fwe). Celà permet de contrôler si des groupes de colonnes respectent l'hypothèse nulle. C'est un contrôle plus strict que celui de SelectFdr. \n",
    "\n",
    "#### [GenericUnivariateSelect](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.GenericUnivariateSelect.html#sklearn.feature_selection.GenericUnivariateSelect)\n",
    "\n",
    "Sélecteur unviarié dont la stratégie est configurable. On peut lui faire utiliser l'ensemble des stratégies ci-dessus. \n",
    "\n",
    "Ces sélecteurs demandent une fonction renvoyant un score et une pvalue en entrée. Les fonctions mises à disposition sont les suivantes:  \n",
    "- Pour la régression [f_regression](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html#sklearn.feature_selection.f_regression), [mutual_info_regression](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression)\n",
    "- Pour la classification \n",
    "[chi2](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2), [f_classif](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif), [mutual_info_classif](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif)  \n",
    "  \n",
    "On peut également définir ses propres fonctions, tant qu'elles renvoient un score et une pvalue.\n",
    "  \n",
    "Les méthodes basées sur le f-test utilisent **le niveau de dépendance linéaire** entre les variables. Les autres permettent de se baser sur **n'importe quel niveau de dépendance** mais nécessitent davantage de données pour être efficaces.  \n",
    "Attention à bien utiliser un score adapté au problème. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sélection avec [SelectFromModel](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel)\n",
    "\n",
    "SelectFromModel est un meta-transformeur qui peut être utilisé avec n'importe quel estimateur donnant l'importance des features via un attribut (par exemple coeff_ ou feature_importances_). On ne garde que les features qui auront une valeur supérieur à un ```threshhold``` donné. On peut utiliser la moyenne ou la mediane des coefficients pour déterminer ce ```threshhold``` (par exemple \"mean/10\"). On peut également utiliser le paramètre ```max_features``` qui limite le nombre de features à sélectionner.  \n",
    "Pour les **modèles linéaires** , les estimateurs les plus efficaces avec SelectFromModel sont ceux basés sur l1 pour éliminer les colonnes dont les coefficients sont nuls. On retrouve donc [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso) pour la régression linéaire et [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression) ainsi que [LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) pour la classification linéaire.  \n",
    "On peut également utiliser l'ensemble des modèles basés sur les **arbres de décision** (et les forêts) avec SelectFromModel pour retirer les colonnes ne contenant pas d'informations utiles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elimination récursive des features\n",
    "\n",
    "En prenant un estimateur donnant l'importance des features comme pour SelectFromModel, l'algorithme va s'appliquer de façon récursive. Il va chercher à obtenir un nombre de plus en plus petit de features en se basant sur l'importance de ces features à chaque itération et en se débarassant de celles qui ont l'importance la plus faible jusqu'à atteindre le nombre de features désiré.  \n",
    "Il existe 2 algorithmes pour effectuer cette élimination récursive : [RFE](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE) qui est l'algorithme de base et [RFECV](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV) qui permet de faire une crossvalidation dans l'algorithme pour sélectionner automatiquement le nombre optimal de features à conserver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sélection séquentielle des features\n",
    "\n",
    "[SequentialFeatureSelector](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html#sklearn.feature_selection.SequentialFeatureSelector) est un algorithme de sélection itératif qui a 2 modes de fonctionnement :\n",
    "- **Forward-SFS** qui commence avec aucune feature et qui va sélectionner la meilleur feature à ajouter à son ensemble de features déjà utilisées à chaque itération. \n",
    "- **Backward-SFS** qui commence avec toutes les features et qui va retirer à chaque itération la feature la moins utile.  \n",
    "\n",
    "L'algorithme s'arrête lorsqu'il atteint le nombre de features fixé par ```n_features_to_select```. Le mode est choisi via la paramètre ```direction```.  \n",
    "Attention : suivant le mode choisi, on a pas le même nombre d'itérations ni le même résultat. \n",
    "\n",
    "La différence avec les modèles précédents est que ce sélecteur n'a pas besoin que le modèle lui donne un attribut de ```coeff_``` ou de ```feature_importance_```. Le sélecteur va directement comparer les modèles pour chaque jeu de features, ce qui fait que cet algorithme est plutôt lent. Il va donc évaluer plusieurs modèles pour chaque itération contrairement aux autres (par exemple RFE) qui n'évalue qu'un seul modèle par itération.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Intégration à un pipeline\n",
    " \n",
    " Il est recommandé d'intégrer la feature selection à un pipeline pour l'appliquer systématiquement aux jeux de données sur lequel on appliquera par la suite le modèle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
